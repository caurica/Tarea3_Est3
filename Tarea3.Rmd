---
title: "Tarea3_ts"
author: "Arath Reyes"
date: "7/11/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(dygraphs)
library(imputeTS)
library(tseries)
library(TSA)
library(nortest)
library(forecast)
library(ggplot2)
library(lmtest)
```

# **Tarea 3:** Modelación ARIMA 

```{r}
library(tsdl)
ts_12 <- tsdl[[12]]
attributes(ts_12)
```
## **Inciso 1:**
Veamos cuáles y cómo son los datos presentados:
```{r}
print(ts_12)
plot(ts_12, main = "Quarterly U.S. new plant/equip. expenditures \n 64 76 billions", col = "red")
tsdisplay(ts_12, main= "Quarterly U.S. new plant/equip. expenditures \n 64 76 billions", col = "red")
```
 
Después de aplicar la función *descompose*, obtuvimos lo siguiente:
 
```{r, align='center', fig.cap= "Figura 1: Serie de Tiempo Descompuesta", fig.height=8, fig.width=12}
# Aplicar decompose
dec_data <- decompose(ts_12)
plot(dec_data) 
```
 
Después de haber imputado datos a la parte relacionada con la tendencia y a la parte aleatoria, obtuvimos el siguiente gráfico, combinando los datos originales, la tendencia y la periodicidad: 

```{r, fig.cap="Figura 2: Datos Originales, Periodicidad y Tendencia", fig.align='center', fig.height=8, fig.width=12}

# Plot de datos originales, tendencia y periodicidad
Tendencia = dec_data$trend
Datos = dec_data$x
Periodicidad = dec_data$seasonal

myVector <- cbind(Datos,Tendencia, Periodicidad)

dygraph(myVector, main = "Data")%>%
  dySeries("Tendencia", drawPoints =  TRUE, color = 'red')

```

A primer vistazo, podemos notar que nuestra serie de tiempo dada por los datos de "Quarterly U.S. new plant/equip. expenditure 64 76 billions" tiene **varianza creciente**, una **tendencia creciente**, así como la **existencia clara de ciclos estacionales.**  Debemos ser minucioso respecto a la última afirmación, pues no es clara, a nuestro parecer, la periodicidad de nuestra serie de tiempo; superficialmente podríamos afirmar que esta periodicidad de 12, 6 o 3 meses.

No obstante, este análisis visual es insuficiente para consolidar nuestras afirmaciones, por ello, procedemos a hacer un análisis estadístico de los mismos.

### Varianza:

Confirmemos nuestra afirmación de que este proceso es **heterocedástico** aplicando una prueba de Breusch-Pagan para homocedasticidad, es decir:
$$\textbf{Test de Breusch-Pagan}$$
$$H_0:\sigma^2 \text{ constante} \quad \text{vs}\quad H_a: \sigma^2\text{ no constante}$$
Por medio del siguiente código:

```{r}
# Observamos las fecha de inicio y final en nuestros datos
print(start(ts_12))
print(end(ts_12))
# Creamos una secuencia con los datos mostrados cada 1/4 (Dado que los datos son trimestrales)
t = seq(1964, 1976+ 3/4, by = 1/4)
bptest(ts_12 ~ t)
```
Podemos observar que el $p$-value es *0.02047*, de esta manera rechazamos $H_{0}$, es decir, tenemos un proceso heterocedástico, lo que significa que nuestro proceso no tiene varianza constante, lo cual confirma nuestra afirmación obtenida por medio del análisis visual.

### Tendencia

Como podemos observar en los gráficos anteriormente mostrados, esta series de tiempo tiene una tendencia creciente, esto se hace explícto al observar la **figura 1**, en la cual la tendencia se muestra como una curva creciente, y esto se confirma al observar la **figura 2**, en la cual la tendencia *(mostrada en color rojo)* se observa como una curva creciente que acompaña a los datos originales.

### Ciclos Estacionales

Observemos primero el ACF y PACF para rescatar información acerca de los ciclos estacionales:
```{r, fig.align='center', fig.height=6, fig.width=6}
par(mfrow = c(2,1))
acf(ts_12)
pacf(ts_12)
```

Visualmente, parece claro que los ciclos estacionales sean de 2 periodos o, equivalentemente, de 6 meses o, 4 periodos o 12 meses. Aún más, viendo el PACF observamos para el 2no punto este se muestra altamente correlacionado, así como el 4to. A pesar de esta evidencia consideremos lo siguiente:


```{r}
p = periodogram(ts_12, main = "Periodograma", col = 4)
# Ordenamos de mayor a menor las estimaciones del periodograma
spec = sort(p$spec, decreasing = TRUE)
spec = spec[1:5]
i = match(spec, p$spec) # Buscamos sus índices en el periodograma
d = p$freq
d = d[i]
cbind(spec, d, i)
d = 1/d
d = floor(d)
d = sort(d)
d #Posibles periodos
```
Dado lo anterior, podemos afirmar que **nuestra serie tiene un ciclo de 2 periodos o de 6 meses.**


Al observar el ACF nos podemos percatar que existen correlaciones anteriores, de esta manera, es tentador afirmar que nuestra serie **NO es estacionaria**, de nuevo, esta interpretación del ACF debe ser corroborada de forma estadística.

### Estacionariedad

Anteriormente afirmamos que nuestra serie de tiempo es no estacionaria, para confirmar dicha premisa aplicaremos dos pruebas de hipótesis para estacionariedad.
$$\textbf{Test de Dickey-Fuller}$$
$$H_0: \text{NO Estacionariedad} \quad \text{vs}\quad H_a:\text{Estacionariedad}$$
 
```{r}
adf.test(ts_12)
```

$$\textbf{Test de  Kwiatkowski-Phillips-Schmidt-Shin}$$
$$H_0: \text{Estacionariedad} \quad \text{vs}\quad H_a:\text{NO Estacionariedad}$$
```{r}
kpss.test(ts_12)
```

Podemos notar que para el test DF fallamos en rechazar la hipótesis nula y para el test KPSS rechazamos la hipótesis nula, lo que en ambos casos nos permite confirmar nuestra hipótesis sobre la no estacionariedad de la serie. 

Busquemos ahora la manera en que, al tranformar la serie, esta se vuelva estacionaria. Consideremos las lambdas de BoxCox por los métodos Guerrero y Loglik.

```{r}
print(paste0("Lambda con método guerrero: ",BoxCox.lambda(ts_12, method = "guerrero")))
print(paste0("Lambda con método loglik: ", BoxCox.lambda(ts_12, method = "loglik")))
```

Dadas las $\lambda$'s obtenidas, tenemos 4 primeras opciones para proceder, las primeras dos consisten en tomar las transformaciones BoxCox asociadas a dichas $\lambda$'s; mientras que las siguientes dos opciones, consisten en considerar las transformaciones usuales sugeridas en nuestras notas, es decir, aplicar la función $\sqrt{X_{t}}$ o la función $\ln(X_{t})$ a nuestros datos. En cualquiera de los casos, después de aplicar la transformación deseada es necesario validar que esta sea homocedastica y estacionaria, sino, es necesario aplicar más transformaciones o regresar a la serie original y decantarse por otra transformación. *Después de muchos intentos consideramos la siguiente transformación de los datos.*

```{r}
data = sqrt(ts_12)
data = diff(diff(data))
data
```

Podemos notar que perdimos dos registros, sin embargo, no es de gran problema pues, en comparación con la cantidad original de registros, no es una pérdida significativa.

Ahora, veamos si nuestra serie transformada es homocedástica y estacionaria.
```{r}
bptest(data~t[3:52])
adf.test(data)
kpss.test(data)
```

Efectivamente, nuestra nueva series es **homocedástica y es estacionaria**. *De aquí en adelante, modelaremos nuestra serie con estos datos transformados.* Veamos los nuevos datos:
```{r}
ts.plot(data, main ="Datos Transformados" ,col = "red", lwd = 2)
acf(data)
pacf(data)
```


## **Inciso 2**
Veamos que los datos a eliminar son los siguientes:
```{r}
datos_eliminados <- c(ts_12[29], ts_12[38], ts_12[39])
datos_eliminados
```
Entonces eliminemos dichos datos:
```{r}
ts_12[29] = NA
ts_12[38] = NA
ts_12[39] = NA
ts_12
```
Imputemos ahora datos nuevos:
```{r}
ts_12_1 = na_interpolation(ts_12)
ts_12_1
```


```{r}
ts_12_2 = na_kalman(ts_12)
ts_12_2
```
Ahora consideremos los datos reemplazados:
```{r}
datos_nuevos1 <- c(ts_12_1[29], ts_12_1[38], ts_12_1[39])
datos_nuevos2 <- c(ts_12_2[29], ts_12_2[38], ts_12_2[39])
print(datos_eliminados)
print(datos_nuevos1)
print(datos_nuevos2)
```
A primera vista parece que los datos imputados por el método del Kalman son más parecidos a los originales que los dados por interpolación, sin embargo, consideremos el *error quadrático medio* de ambas estimaciones y así confirmar nuestra declaración anterior.

```{r}
MSE1 = (1/3)*mean((datos_eliminados - datos_nuevos1)^2)
MSE2 = (1/3)*mean((datos_eliminados - datos_nuevos2)^2)
print(paste0("Error Cuadrático Medio de Estimación por Interpolación: ", MSE1))
print(paste0("Error Cuadrático Medio de Estimación por Kalman: ", MSE2))
```

Con lo anterior confirmamos que el método de Kalman fue mejor para estimar nuestros datos eliminados.

## Inciso 3:
Primero observemos el ACF y PACF para notar cuantos parámetros significantes tenemos:
```{r}
tsdisplay(data)
```
Observamos algo raro en el ACF pues todas las observaciones se salen de nuestras bandas de confianza, esto debido a que tenemos un proceso estacionario con ciclos estacionales. Además, podemos observar que siguen cierto comportamiento, por lo que propondremos los siguientes modelos:

```{r}
#Modelo 1
#SARIMA(1,0,1)(0,1,1)[4]
fit1_data <- arima(data, c(1, 0, 1), seasonal = list(order = c(0, 1, 1), period = 4))
#Modelos 2
#SARIMA(0,0,1)(1,1,1)[4] 
fit2_data <- arima(data, c(0, 0, 1), seasonal = list(order = c(1, 1, 1), period = 4))
#Modelo 3
#SARIMA(0,0,1)(0,1,0)[4] 
fit3_data <- arima(data, c(0, 0, 1), seasonal = list(order = c(0, 1, 0), period = 4))
#Modelo 5
#SARIMA(1,0,1)(1,1,1)[4]
fit5_data  <- arima(data, c(1, 0, 1), seasonal = list(order = c(1, 1, 1), period = 4))
```

Ahora usemos la función *auto.arima()* para proponer un cuarto moedelo:

```{r}
fit4_data <- auto.arima(data)
summary(fit4_data)
```
Observemos que tenemos un modelos *SARIMA* con coeficientes $p=0, P=0, d=0, D=1, q=1$ y $Q=1$. Notemos que la $D=1$ nos inidica cuantas diferencias necesitamos para estabilizar los ciclos estacionales.

Ahora veamos graficamente los datos discrepantes que tenemos en cada modelo:

```{r, datos discrepantes}
#Modelo1
plot(fit1_data$residuals, type="p", col="blue", ylab="", xlab="", main="Modelo 1", lwd=2)
abline(h=3*(var(fit1_data$residuals)), col="red", lwd=2)
abline(h=-3*(var(fit1_data$residuals)), col="red", lwd=2)

#Modelo 2
plot(fit2_data$residuals, type="p", col="blue", ylab="", xlab="", main="Modelo 2", lwd=2)
abline(h=3*(var(fit2_data$residuals)), col="red", lwd=2)
abline(h=-3*(var(fit2_data$residuals)), col="red", lwd=2)

#Modelo 3
plot(fit3_data$residuals, type="p", col="blue", ylab="", xlab="", main="Modelo 3", lwd=2)
abline(h=3*(var(fit3_data$residuals)), col="red", lwd=2)
abline(h=-3*(var(fit3_data$residuals)), col="red", lwd=2)

#Modelo 4
plot(fit4_data$residuals, type="p", col="blue", ylab="", xlab="", main="Modelo 4", lwd=2)
abline(h=3*(var(fit4_data$residuals)), col="red", lwd=2)
abline(h=-3*(var(fit4_data$residuals)), col="red", lwd=2)

#Modelo 5
plot(fit5_data$residuals, type="p", col="blue", ylab="", xlab="", main="Modelo 5", lwd=2)
abline(h=3*(var(fit5_data$residuals)), col="red", lwd=2)
abline(h=-3*(var(fit5_data$residuals)), col="red", lwd=2)
```
Podemos notar que tenemos una gran cantidad de datos discrepantes, por lo que no podemos garantizar que algun modelo es mejor que otro. Entonces hagamos una comparación de criterios de bondad de ajuste, es decir veamos el AIC, BIC, ME, MAE, RMSE y  el número de parámetros:

```{r}
comparar_1=cbind("SARIMA(1,0,1)(0,1,1)[4]",fit1_data$aic,BIC(fit1_data), mean(fit1_data$residuals), mean(abs(fit1_data$residuals)), sqrt(mean((fit1_data$residuals)^2)), 4)

comparar_2=cbind("SARIMA(0,0,1)(1,1,1)[4]",fit2_data$aic,BIC(fit2_data), mean(fit2_data$residuals), mean(abs(fit2_data$residuals)), sqrt(mean((fit2_data$residuals)^2)), 4)

comparar_3=cbind("SARIMA(0,0,1)(0,1,0)[4]",fit3_data$aic, BIC(fit3_data), mean(fit3_data$residuals), mean(abs(fit3_data$residuals)), sqrt(mean((fit3_data$residuals)^2)), 4)

comparar_4=cbind("SARIMA(0,0,1)(0,1,1)[4]", fit4_data$aic, BIC(fit4_data), mean(fit4_data$residuals), mean(abs(fit4_data$residuals)), sqrt(mean((fit4_data$residuals)^2)), 4)

comparar_5=cbind("SARIMA(1,0,1)(1,1,1)[4]", fit5_data$aic, BIC(fit5_data), mean(fit5_data$residuals), mean(abs(fit5_data$residuals)), sqrt(mean((fit5_data$residuals)^2)), 4)

nombres=cbind("AJUSTE", "AIC", "BIC","ME","MAE", "RMSE","#Paramatros")

resultados<-rbind(comparar_1,comparar_2,comparar_3, comparar_4, comparar_5)
resultados<-as.table(resultados)
colnames(resultados)=c("AJUSTE", "AIC", "BIC","ME","MAE", "RMSE","#Parametros")
rownames(resultados)=c("", "", "", "", "")

(resultados)
```
Considerando el AIC, notemos que el *modelo 1* se ajusta mejor y tomando en cuenta el BIC tenemos que el *modelo 4* se ajusta mejor. Pero considerando los errores tenemos que el *modelo 1* se ajusta mejor. Ahora verifiquemos los supuestos para elegir el mejor modelo.

## Normalidad
Primero veamos *qqnorm*:
```{r}
#Modelo 1
qqnorm(fit1_data$residuals, main = "Normal QQ-Plot Modelo 1")
qqline(fit1_data$residuals, col="red", lwd=2)
#Modelo 2
qqnorm(fit2_data$residuals, main = "Normal QQ-Plot Modelo 2")
qqline(fit2_data$residuals, col="red", lwd=2)
#Modelo 3
qqnorm(fit3_data$residuals, main = "Normal QQ-Plot Modelo 3")
qqline(fit3_data$residuals, col="red", lwd=2)
#Modelo 4
qqnorm(fit4_data$residuals, main = "Normal QQ-Plot Modelo 4")
qqline(fit4_data$residuals, col="red", lwd=2)
#Modelo 5
qqnorm(fit5_data$residuals, main = "Normal QQ-Plot Modelo 5")
qqline(fit5_data$residuals, col="red", lwd=2)
```
A primera vista, podemos observar que ningun modelo cumple con el supuesto de normalidad, pero para confirmar esto realizaremos algunos test de normalidad.

$$\textbf{Test de  Anderson-Darling}$$
$$H_0: \text{Los datos siguen una distribución normal} \quad \text{vs}\quad H_a:\text{Los datos no siguen una distribución normal}$$
```{r}
#Modelo 1
ad.test(fit1_data$residuals)
#Modelo 2
ad.test(fit2_data$residuals)
#Modelo 3
ad.test(fit3_data$residuals)
#Modelo 4
ad.test(fit4_data$residuals)
#Modelo 5
ad.test(fit5_data$residuals)
```
Notemos que para todos los modelos el $p-value > \alpha = 0.05$, entonces aceptamos $H_0$, por lo que la prueba de Aderson-Darling nos dice que nuestros residuos siguen una distribución normal. 

$$\textbf{Test de  Shapiro-Wilks}$$
$$H_0: \text{La distribución es normal} \quad \text{vs}\quad H_a:\text{La distribución no es normal}$$

```{r}
#Modelo 1
shapiro.test(fit1_data$residuals)
#Modelo 2
shapiro.test(fit2_data$residuals)
#Modelo 3
shapiro.test(fit3_data$residuals)
#Modelo 4
shapiro.test(fit4_data$residuals)
#Modelo 5
shapiro.test(fit5_data$residuals)
```
Para los *modelos 2* y *4* $p-value < \alpha = 0.05$ entonces rechazamos $H_0$, por lo que la prueba de Shapiro-Wilks nos dice que nuestros datos no siguen una distribución normal. Pero para los *modelos 1, 3* y *5* $p-value > \alpha = 0.05$ entonces aceptamos $H_0$, por lo que la prueba de Shapiro-Wilks nos dice que nuestros residuales siguen una distribución normal.

$$\textbf{Test de  Jarque-Bera}$$
$$H_0: \text{La distribución es normal} \quad \text{vs}\quad H_a:\text{La distribución no es normal}$$
```{r}
#Modelo 1
jarque.bera.test(fit1_data$residuals)
#Modelo 2
jarque.bera.test(fit2_data$residuals)
#Modelo 3
jarque.bera.test(fit3_data$residuals)
#Modelo 4
jarque.bera.test(fit4_data$residuals)
#Modelo 5
jarque.bera.test(fit5_data$residuals)
```
Notemos que para los *modelos 1, 2* y *4* el $p-value < \alpha = 0.05$ entonces rechazamos $H_0$, por lo que la prueba de Jarque-Bera nos dice que nuestros residuales no siguen una distribución normal. Pero para el *modelo 3* y *5* el $p-value > \alpha = 0.05$ entonces aceptamos $H_0$, por lo que la prueba de Jarque-Bera nos dice que nuestros residuales siguen una distribución normal.

Los modelos que pasaron todas las pruebas de normalidad fueron el *3* y *5*. Ahora veamos los demás supuestos.

## Varianza constante

Apliquemos las siguientes pruebas:

$$\textbf{Test de Breusch-Pagan}$$
$$H_0:\sigma^2 \text{ constante} \quad \text{vs}\quad H_a: \sigma^2\text{ no constante}$$

```{r}
#Modelo 1
Y1 <- as.numeric(fit1_data$residuals)
X1 <- 1:length(fit1_data$residuals)
bptest(Y1 ~ X1)
#Modelo 2
Y2 <- as.numeric(fit2_data$residuals)
X2 <- 1:length(fit2_data$residuals)
bptest(Y2 ~ X2)
#Modelo 3
Y3 <- as.numeric(fit3_data$residuals)
X3 <- 1:length(fit3_data$residuals)
bptest(Y3 ~ X3)
#Modelo 4
Y4 <- as.numeric(fit4_data$residuals)
X4 <- 1:length(fit4_data$residuals)
bptest(Y4 ~ X4)
#Modelo 5
Y5 <- as.numeric(fit5_data$residuals)
X5 <- 1:length(fit5_data$residuals)
bptest(Y5 ~ X5)
```
Notemos que para todos nuestros modelos el $p-value > \alpha = 0.05$, entonces aceptamos $H_0$, por lo que la prueba de Breusch-Pagan nos dice que nuestros residuales tienen varianza constante.

## Media cero

$$\textbf{Test T-cuadrada de Hotelling}$$
$$H_0: \text{La media es igual a 0} \quad \text{vs}\quad H_a: \text{ La media no es igual a 0}$$
```{r}
#Modelo 1
t.test(fit1_data$residuals,mu=0)
#Modelo 2
t.test(fit2_data$residuals,mu=0)
#Modelo 3
t.test(fit3_data$residuals,mu=0)
#Modelo 4
t.test(fit4_data$residuals,mu=0)
#Modelo 5
t.test(fit5_data$residuals,mu=0)
```
Observemos que para todos nuestros modelos el $p-value > \alpha = 0.05$, entonces aceptamos $H_0$, por lo que la prueba T-cuadrada de Hotelling nos dice que tenemos media cero.

## Independencia

Primero veamos graficamente los residuales:

```{r}
#Modelo 1
ggtsdisplay(fit1_data$residuals,main="Residuales Modelo 1")
#Modelo 2
ggtsdisplay(fit2_data$residuals,main="Residuales Modelo 2")
#Modelo 3
ggtsdisplay(fit3_data$residuals,main="Residuales Modelo 3")
#Modelo 4
ggtsdisplay(fit4_data$residuals,main="Residuales Modelo 4")
#Modelo 5
ggtsdisplay(fit5_data$residuals,main="Residuales Modelo 5")
```

Para el *modelo 1* y *2* notemos que en $lag=4$ ligeramente se sale nuestro dato del intervalo de confianza por lo que aplicaremos la prueba de Box-Pierce en dicho lag. Para el *modelo 3* aplicaremos la misma prueba pero en $lag=8$ y para los *modelo 4* y *5* en $lag=5$.

$$\textbf{Test de Box-Pierce}$$
$$H_0: \text{Independencia} \quad \text{vs}\quad H_a: \text{Dependencia}$$
```{r}
#Modelo 1
Box.test(fit1_data$residuals, lag = 4)
#Modelo 2
Box.test(fit2_data$residuals, lag = 4)
#Modelo 3
Box.test(fit3_data$residuals, lag = 8)
#Modelo 4
Box.test(fit4_data$residuals, lag = 5)
#Modelo 5
Box.test(fit5_data$residuals, lag = 5)
```
Notemos que para todos los modelos el $p-value > \alpha = 0.05$, entonces aceptamos $H_0$, por lo que la prueba de Box-Pierce nos dice que tenemos independencia en los residuales de todos nuestros modelos.

Ahora, relicemos la siguiente prueba:
$$\textbf{Test de Ljung-Box}$$
$$H_0: \text{Los datos se distribuyen de forma independiente} \quad \text{vs}\quad H_a: \text{Los datos no se distribuyen de forma independiente}$$
```{r}
#Modelo 1
tsdiag(fit1_data)
#Modelo 2
tsdiag(fit2_data)
#Modelo 3
tsdiag(fit3_data)
#Modelo 4
tsdiag(fit4_data)
#Modelo 5
tsdiag(fit5_data)
```
Notemos que para todos nuestros modelos los $lags$ se encuentra por encima de nuestra $\alpha = 0.05$ por lo que aceptamos la hipótesis nula, es decir si tenemos independencia en todos los modelos.

Ahora calculemos los intervalos de confianza para ver si los parámetros son significativos.

```{r}
#Modelo 1
confint(fit1_data)
#Modelo 2
confint(fit2_data)
#Modelo 3
confint(fit3_data)
#Modelo 4
confint(fit4_data)
#Modelo 5
confint(fit5_data)
```
Para los *modelos 1, 3* y *4* no tenemos al cero en nuestros intervalos de confianza, es decir todos los parámetros son significativos. Pero para los *modelos 2* y *5* si tenemos al cero en los intervalos por lo que tenemos parámetros no significativos.

Resumiendo tenemos los siguientes resultados para la verificación de supuestos:

Table: Análisis de los residuales

|          |**Normalidad**|**Varianza constante**|**Media cero**|**Independencia**|
|----------|--------------|----------------------|--------------|-----------------|
|*Modelo 1*|   $\times$   |      $\checkmark$    | $\checkmark$ |   $\checkmark$  | 
|*Modelo 2*|   $\times$   |     $\checkmark$     | $\checkmark$ |   $\checkmark$  |
|*Modelo 3*|$\checkmark$  |     $\checkmark$     | $\checkmark$ |   $\checkmark$  |
|*Modelo 4*|   $\times$   |     $\checkmark$     | $\checkmark$ |   $\checkmark$  | 
|*Modelo 5*| $\checkmark$ |     $\checkmark$     | $\checkmark$ |   $\checkmark$  |

Podemos ver que el *modelo 3* pasa todos los supuestos, sin embargo tiene los peores valores para el *MAE*, *RMSE*, *AIC* y *BIC*. Por otro lado, el *modelo 5* tambien pasa todos los supuestos pero tenemos parámetros NO significativos, pues en el intervalo de confianza *sar1* tenemos al cero. El *modelo 2* tambien contiene parámetros NO significativos y su *AIC* y *BIC* no son tan bueno. 

Ahora, comparando el *modelo 1* y el *modelo 4*, podemos ver que el primero tiene mejores valores para *AIC*, *ME*, *MAE* y *RMSE* que el cuarto modelo. Por lo tanto nos quedamos con el **modelo 1**.


## Inciso 4:

Ya que hemos determinado que (1,2,1)(0,1,1)[4] se ajusta adecuadamente, seguiramos trabajando con éste


```{r}


fit1 <- arima(data2, order=c(1,2,1), seasonal=list(order=c(0,1,1), period=4))
#hacemos 2 diferenciaciones manualmente para obtener estacionaridad

fit1
```


Ahora hacemos las predicciones para los siguientes 2 años:


```{r}

### Predict

pred <- predict(fit1, n.ahead=8)$pred
pred
pred= pred^2
pred1 <- predict(fit1, n.ahead = 24)
tl <- pred1$pred - 1.96 * pred1$se
tu <- pred1$pred + 1.96 * pred1$se
tl <- (tl)^2
tu <- (tu)^2

plot(forecast(ts_12, 4))
ts.plot(ts_12_2, xlim= c(1964, 1979), ylim=c(9,50), main= "predicción")
points(pred, type="l", col= 3)
points(tl, type= "l", col= "red")
points(tu, type= "l", col= "red")
```



